{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95cce0cc",
   "metadata": {},
   "source": [
    "# Hybrid Framework Approaches: Combining AI Agent Systems\n",
    "\n",
    "This notebook demonstrates how to build hybrid AI agent systems that combine multiple frameworks. These examples show how to leverage the strengths of different frameworks while overcoming their individual limitations.\n",
    "\n",
    "## Helpful Resources\n",
    "\n",
    "- [LangChain AI Agent Development Guide](https://python.langchain.com/docs/integrations/agents/)\n",
    "- [Microsoft Research: Foundation of Autonomous Agents](https://www.microsoft.com/en-us/research/group/autonomous-systems-group-robotics/articles/foundation-of-autonomous-agents/)\n",
    "- [Autonomous Agent Architectures Blog](https://lilianweng.github.io/posts/2023-06-23-agent/)\n",
    "- [Building LLM-powered Autonomous Agents](https://blog.langchain.dev/building-llm-powered-autonomous-agents/)\n",
    "- [Stanford Agent Framework Overview](https://crfm.stanford.edu/2023/06/16/agent-framework.html)\n",
    "- [LlamaIndex Agent Framework](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "First, let's install all the required packages for the different frameworks we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ccaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langgraph crewai pyautogen langchain_openai\n",
    "!pip install wikipedia transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f981c9",
   "metadata": {},
   "source": [
    "## Setting API Keys and Configurations\n",
    "\n",
    "For these examples to work, you'll need to set your API keys for OpenAI or Azure OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768f620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: OpenAI Setup\n",
    "# Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"  # Replace with your actual API key\n",
    "\n",
    "# Option 2: Azure OpenAI Setup\n",
    "# Set your Azure OpenAI variables\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your-azure-openai-api-key\"  # Replace with your Azure API key\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-resource-name.openai.azure.com/\"  # Replace with your endpoint\n",
    "\n",
    "# Helper function to create appropriate clients based on provider choice\n",
    "def create_clients(provider=\"openai\"):\n",
    "    \"\"\"Create OpenAI or Azure OpenAI clients for different frameworks.\"\"\"\n",
    "    from langchain_openai import ChatOpenAI, AzureChatOpenAI\n",
    "    \n",
    "    if provider == \"azure\":\n",
    "        # Azure OpenAI setup\n",
    "        langchain_llm = AzureChatOpenAI(\n",
    "            azure_deployment=\"gpt-4\",  # The deployment name you chose\n",
    "            openai_api_version=\"2023-05-15\",\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Azure OpenAI config for AutoGen\n",
    "        autogen_config = {\n",
    "            \"config_list\": [{\n",
    "                'model': 'gpt-4',  # Your deployment name\n",
    "                'api_type': 'azure',\n",
    "                'api_version': '2023-05-15',\n",
    "                'api_key': os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\"),\n",
    "                'api_base': os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\")\n",
    "            }]\n",
    "        }\n",
    "    else:  # Default to OpenAI\n",
    "        # Standard OpenAI setup\n",
    "        langchain_llm = ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0.7,\n",
    "            api_key=os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "        )\n",
    "        \n",
    "        # OpenAI config for AutoGen\n",
    "        autogen_config = {\n",
    "            \"config_list\": [{\n",
    "                'model': 'gpt-4',\n",
    "                'api_key': os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "            }]\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"langchain_llm\": langchain_llm,\n",
    "        \"autogen_config\": autogen_config\n",
    "    }\n",
    "\n",
    "# Choose your provider\n",
    "provider = \"openai\"  # Change to \"azure\" to use Azure OpenAI\n",
    "\n",
    "# Get the appropriate clients\n",
    "clients = create_clients(provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19addaef",
   "metadata": {},
   "source": [
    "## Pattern 1: LangChain + LangGraph\n",
    "\n",
    "This example demonstrates how to combine LangChain's tools with LangGraph's advanced control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9973bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import TypedDict, List, Dict\n",
    "\n",
    "# This is a mock implementation - replace with actual API key if using\n",
    "# search = GoogleSearchAPIWrapper()\n",
    "class MockSearch:\n",
    "    def run(self, query):\n",
    "        return f\"Mock search results for: {query}\"\n",
    "\n",
    "search = MockSearch()\n",
    "\n",
    "# Use LangChain for tools and integrations\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for searching the internet\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define our state using TypedDict\n",
    "class ResearchState(TypedDict):\n",
    "    query: str\n",
    "    search_results: str\n",
    "    refined_results: str\n",
    "    final_answer: str\n",
    "\n",
    "# Define nodes in the graph\n",
    "def search_for_information(state: ResearchState):\n",
    "    \"\"\"Search for information based on the query.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "    search_results = search.run(query)\n",
    "    state[\"search_results\"] = search_results\n",
    "    return state\n",
    "\n",
    "def analyze_search_results(state: ResearchState):\n",
    "    \"\"\"Analyze and refine search results.\"\"\"\n",
    "    llm = clients[\"langchain_llm\"]  # Use the selected LLM client\n",
    "    prompt = f\"\"\"Analyze these search results and extract the most relevant information:\n",
    "    Query: {state['query']}\n",
    "    Search Results: {state['search_results']}\n",
    "    \"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"refined_results\"] = response.content\n",
    "    return state\n",
    "\n",
    "def formulate_answer(state: ResearchState):\n",
    "    \"\"\"Generate a final answer based on the refined results.\"\"\"\n",
    "    llm = clients[\"langchain_llm\"]  # Use the selected LLM client\n",
    "    prompt = f\"\"\"Based on the refined information, provide a comprehensive answer to the query:\n",
    "    Query: {state['query']}\n",
    "    Refined Information: {state['refined_results']}\n",
    "    \"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"final_answer\"] = response.content\n",
    "    return state\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"search\", search_for_information)\n",
    "workflow.add_node(\"analyze\", analyze_search_results)\n",
    "workflow.add_node(\"answer\", formulate_answer)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(\"search\", \"analyze\")\n",
    "workflow.add_edge(\"analyze\", \"answer\")\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"search\")\n",
    "\n",
    "# Compile the graph\n",
    "research_graph = workflow.compile()\n",
    "\n",
    "# Example execution (commented out)\n",
    "'''\n",
    "initial_state = {\"query\": \"What are the latest advancements in fusion energy?\", \"search_results\": \"\", \"refined_results\": \"\", \"final_answer\": \"\"}\n",
    "result = research_graph.invoke(initial_state)\n",
    "print(\"Final Answer:\")\n",
    "print(result[\"final_answer\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cd764",
   "metadata": {},
   "source": [
    "## Setting API Keys\n",
    "\n",
    "For these examples to work, you'll need to set your API keys for the LLM providers you're using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ae6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f640dc",
   "metadata": {},
   "source": [
    "## Example 1: LangChain + LangGraph Hybrid\n",
    "\n",
    "This example combines LangChain's tools and retrievers with LangGraph's workflow control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88353c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "\n",
    "# Define our state type\n",
    "class ResearchState(TypedDict):\n",
    "    query: str\n",
    "    needs_research: bool\n",
    "    research_results: str\n",
    "    follow_up_questions: List[str]\n",
    "    answer: str\n",
    "\n",
    "# Initialize LangChain tools\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "# Define LangGraph nodes that use LangChain components\n",
    "\n",
    "def analyze_query(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Determine if the query needs research.\"\"\"\n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=f\"Does this query require factual research? Answer with just Yes or No: {state['query']}\")\n",
    "    ])\n",
    "    state[\"needs_research\"] = \"yes\" in response.content.lower()\n",
    "    return state\n",
    "\n",
    "def perform_research(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Use LangChain's Wikipedia tool to gather information.\"\"\"\n",
    "    try:\n",
    "        research_results = wikipedia.run(state[\"query\"])\n",
    "        state[\"research_results\"] = research_results\n",
    "    except Exception as e:\n",
    "        state[\"research_results\"] = f\"Error performing research: {str(e)}\"\n",
    "    return state\n",
    "\n",
    "def generate_questions(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Generate follow-up questions based on research.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Based on this research about: {state['query']}\n",
    "    Research findings: {state['research_results']}\n",
    "    \n",
    "    Generate 3 specific follow-up questions that would help explore this topic further.\n",
    "    Return ONLY the questions as a numbered list.\n",
    "    \"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # Parse the questions\n",
    "    questions = response.content.strip().split('\\n')\n",
    "    state[\"follow_up_questions\"] = questions\n",
    "    return state\n",
    "\n",
    "def formulate_answer(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Create a comprehensive answer using research results.\"\"\"\n",
    "    if state[\"needs_research\"]:\n",
    "        prompt = f\"\"\"\n",
    "        Query: {state['query']}\n",
    "        Research findings: {state['research_results']}\n",
    "        \n",
    "        Based on this information, provide a comprehensive and accurate answer to the query.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"Answer this query based on your existing knowledge: {state['query']}\"\n",
    "        \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"answer\"] = response.content\n",
    "    return state\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"analyze\", analyze_query)\n",
    "workflow.add_node(\"research\", perform_research)\n",
    "workflow.add_node(\"generate_questions\", generate_questions)\n",
    "workflow.add_node(\"answer\", formulate_answer)\n",
    "\n",
    "# Add conditional edges\n",
    "workflow.add_edge(\"analyze\", \"research\", condition=lambda s: s[\"needs_research\"])\n",
    "workflow.add_edge(\"analyze\", \"answer\", condition=lambda s: not s[\"needs_research\"])\n",
    "workflow.add_edge(\"research\", \"generate_questions\")\n",
    "workflow.add_edge(\"generate_questions\", \"answer\")\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(\"analyze\")\n",
    "\n",
    "# Compile graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Run the hybrid workflow\n",
    "result = graph.invoke({\n",
    "    \"query\": \"What were the major contributions of Marie Curie to science?\",\n",
    "    \"needs_research\": False,\n",
    "    \"research_results\": \"\",\n",
    "    \"follow_up_questions\": [],\n",
    "    \"answer\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"Needed research: {result['needs_research']}\")\n",
    "print(\"\\nFollow-up questions:\")\n",
    "for q in result['follow_up_questions']:\n",
    "    print(f\"- {q}\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe8bbc",
   "metadata": {},
   "source": [
    "## Example 2: AutoGen + CrewAI Hybrid\n",
    "\n",
    "This example combines AutoGen's conversational capabilities with CrewAI's role-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6b4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from crewai import Agent as CrewAgent, Task, Crew\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Configure AutoGen\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': os.environ.get(\"OPENAI_API_KEY\")\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set up AutoGen conversation agents\n",
    "autogen_assistant = autogen.AssistantAgent(\n",
    "    name=\"Conversational_Assistant\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    system_message=\"You help users refine their research questions to be specific and answerable.\"\n",
    ")\n",
    "\n",
    "autogen_user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_Proxy\",\n",
    "    human_input_mode=\"NEVER\"  # No human in the loop for this example\n",
    ")\n",
    "\n",
    "# Function to use AutoGen for query refinement\n",
    "def refine_query_with_autogen(query):\n",
    "    autogen_user_proxy.initiate_chat(\n",
    "        autogen_assistant,\n",
    "        message=f\"Please help refine this research question to be more specific and answerable: '{query}'\"\n",
    "    )\n",
    "    # Extract the refined query from the last assistant message\n",
    "    conversation = autogen_user_proxy.chat_messages[autogen_assistant]\n",
    "    refined_query = conversation[-1][\"content\"]\n",
    "    return refined_query\n",
    "\n",
    "# Set up CrewAI agents\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "researcher = CrewAgent(\n",
    "    role=\"Research Specialist\",\n",
    "    goal=\"Find comprehensive and accurate information\",\n",
    "    backstory=\"You are an expert researcher with a talent for finding relevant information.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "writer = CrewAgent(\n",
    "    role=\"Content Writer\",\n",
    "    goal=\"Create engaging and informative content\",\n",
    "    backstory=\"You are a skilled writer who transforms complex information into clear, engaging content.\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Function to use CrewAI for research and writing\n",
    "def research_and_write_with_crewai(query):\n",
    "    # Define tasks for the crew\n",
    "    research_task = Task(\n",
    "        description=f\"Research this topic thoroughly: {query}\",\n",
    "        expected_output=\"Comprehensive research findings with citations\",\n",
    "        agent=researcher\n",
    "    )\n",
    "    \n",
    "    writing_task = Task(\n",
    "        description=\"Create an informative article based on the research findings\",\n",
    "        expected_output=\"Well-structured article with introduction, main points, and conclusion\",\n",
    "        agent=writer,\n",
    "        context=[research_task]\n",
    "    )\n",
    "    \n",
    "    # Create and run the crew\n",
    "    crew = Crew(\n",
    "        agents=[researcher, writer],\n",
    "        tasks=[research_task, writing_task],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    result = crew.kickoff()\n",
    "    return result\n",
    "\n",
    "# Hybrid workflow combining AutoGen and CrewAI\n",
    "def hybrid_research_process(query):\n",
    "    print(f\"Original query: {query}\")\n",
    "    \n",
    "    # Step 1: Use AutoGen to refine the query through conversation\n",
    "    refined_query = refine_query_with_autogen(query)\n",
    "    print(f\"\\nRefined query: {refined_query}\")\n",
    "    \n",
    "    # Step 2: Use CrewAI to research and write content\n",
    "    result = research_and_write_with_crewai(refined_query)\n",
    "    print(\"\\nFinal result:\")\n",
    "    print(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "hybrid_research_process(\"Tell me about AI agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25caa49e",
   "metadata": {},
   "source": [
    "## Example 3: Custom Orchestration Layer\n",
    "\n",
    "This example demonstrates a custom orchestration layer that selectively uses different frameworks based on the task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "import autogen\n",
    "from enum import Enum\n",
    "\n",
    "# Define task types\n",
    "class TaskType(Enum):\n",
    "    RESEARCH = \"research\"\n",
    "    CONVERSATION = \"conversation\"\n",
    "    CODE_GENERATION = \"code_generation\"\n",
    "    CONTENT_CREATION = \"content_creation\"\n",
    "\n",
    "class Task:\n",
    "    def __init__(self, type: TaskType, query: str, context=None):\n",
    "        self.type = type\n",
    "        self.query = query\n",
    "        self.context = context or {}\n",
    "\n",
    "class CustomOrchestrator:\n",
    "    def __init__(self):\n",
    "        # Initialize LLM\n",
    "        self.llm = ChatOpenAI()\n",
    "        \n",
    "        # Set up LangChain tools\n",
    "        self.wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "        \n",
    "        # Set up AutoGen agents\n",
    "        config_list = [\n",
    "            {\n",
    "                'model': 'gpt-4',\n",
    "                'api_key': os.environ.get(\"OPENAI_API_KEY\")\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.coding_agent = autogen.AssistantAgent(\n",
    "            name=\"Coding_Assistant\",\n",
    "            llm_config={\"config_list\": config_list},\n",
    "            system_message=\"You are an expert programmer. Write clean, efficient code.\"\n",
    "        )\n",
    "        \n",
    "        self.user_proxy = autogen.UserProxyAgent(\n",
    "            name=\"User_Proxy\",\n",
    "            human_input_mode=\"NEVER\",\n",
    "            code_execution_config={\"work_dir\": \"coding_workspace\", \"use_docker\": False}\n",
    "        )\n",
    "    \n",
    "    def process_task(self, task: Task):\n",
    "        \"\"\"Process a task using the appropriate framework.\"\"\"\n",
    "        if task.type == TaskType.RESEARCH:\n",
    "            return self._handle_research_task(task)\n",
    "        elif task.type == TaskType.CONVERSATION:\n",
    "            return self._handle_conversation_task(task)\n",
    "        elif task.type == TaskType.CODE_GENERATION:\n",
    "            return self._handle_code_generation_task(task)\n",
    "        elif task.type == TaskType.CONTENT_CREATION:\n",
    "            return self._handle_content_creation_task(task)\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown task type: {task.type}\"}\n",
    "    \n",
    "    def _handle_research_task(self, task: Task):\n",
    "        \"\"\"Handle research using LangChain tools.\"\"\"\n",
    "        try:\n",
    "            research_results = self.wikipedia.run(task.query)\n",
    "            \n",
    "            # Summarize the research results\n",
    "            prompt = f\"\"\"\n",
    "            Summarize the following research findings about: {task.query}\n",
    "            \n",
    "            {research_results}\n",
    "            \n",
    "            Provide a concise but comprehensive summary.\n",
    "            \"\"\"\n",
    "            \n",
    "            summary_response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            return {\n",
    "                \"raw_results\": research_results,\n",
    "                \"summary\": summary_response.content\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Research error: {str(e)}\"}\n",
    "    \n",
    "    def _handle_conversation_task(self, task: Task):\n",
    "        \"\"\"Handle conversation using direct LLM calls.\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke([HumanMessage(content=task.query)])\n",
    "            return {\"response\": response.content}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Conversation error: {str(e)}\"}\n",
    "    \n",
    "    def _handle_code_generation_task(self, task: Task):\n",
    "        \"\"\"Handle code generation using AutoGen.\"\"\"\n",
    "        try:\n",
    "            self.user_proxy.initiate_chat(\n",
    "                self.coding_agent,\n",
    "                message=task.query\n",
    "            )\n",
    "            \n",
    "            # Extract the generated code from the conversation\n",
    "            conversation = self.user_proxy.chat_messages[self.coding_agent]\n",
    "            code_response = conversation[-1][\"content\"]\n",
    "            \n",
    "            return {\"generated_code\": code_response}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Code generation error: {str(e)}\"}\n",
    "    \n",
    "    def _handle_content_creation_task(self, task: Task):\n",
    "        \"\"\"Handle content creation using a tailored prompt.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"\n",
    "            Create content on the following topic: {task.query}\n",
    "            \n",
    "            Style: {task.context.get('style', 'informative')}\n",
    "            Length: {task.context.get('length', 'medium')}\n",
    "            Target audience: {task.context.get('audience', 'general')}\n",
    "            \n",
    "            Create well-structured content that is engaging and valuable for the target audience.\n",
    "            \"\"\"\n",
    "            \n",
    "            response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "            return {\"content\": response.content}\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Content creation error: {str(e)}\"}\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = CustomOrchestrator()\n",
    "\n",
    "# Example usage for different task types\n",
    "\n",
    "# Research task\n",
    "research_task = Task(\n",
    "    type=TaskType.RESEARCH,\n",
    "    query=\"What are the environmental impacts of quantum computing?\"\n",
    ")\n",
    "research_result = orchestrator.process_task(research_task)\n",
    "print(\"\\n=== RESEARCH TASK RESULT ===\")\n",
    "print(research_result.get('summary', research_result.get('error')))\n",
    "\n",
    "# Code generation task\n",
    "code_task = Task(\n",
    "    type=TaskType.CODE_GENERATION,\n",
    "    query=\"Write a Python function to find the prime numbers in a given range using the Sieve of Eratosthenes algorithm.\"\n",
    ")\n",
    "code_result = orchestrator.process_task(code_task)\n",
    "print(\"\\n=== CODE GENERATION TASK RESULT ===\")\n",
    "print(code_result.get('generated_code', code_result.get('error')))\n",
    "\n",
    "# Content creation task\n",
    "content_task = Task(\n",
    "    type=TaskType.CONTENT_CREATION,\n",
    "    query=\"The future of renewable energy\",\n",
    "    context={\"style\": \"authoritative\", \"length\": \"short\", \"audience\": \"policymakers\"}\n",
    ")\n",
    "content_result = orchestrator.process_task(content_task)\n",
    "print(\"\\n=== CONTENT CREATION TASK RESULT ===\")\n",
    "print(content_result.get('content', content_result.get('error')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6559ef5",
   "metadata": {},
   "source": [
    "## Example 4: Custom Agent with Performance Optimizations\n",
    "\n",
    "This example demonstrates a custom agent implementation with various performance optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a0b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import time\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class OptimizedAgent:\n",
    "    def __init__(self, name, model=\"gpt-4\", cache_enabled=True):\n",
    "        self.name = name\n",
    "        self.llm = ChatOpenAI(model=model)\n",
    "        self.cache = {}\n",
    "        self.cache_enabled = cache_enabled\n",
    "        self.request_count = 0\n",
    "        self.token_count = 0\n",
    "        \n",
    "    def _get_cache_key(self, prompt):\n",
    "        \"\"\"Generate a cache key for a prompt.\"\"\"\n",
    "        return hashlib.md5(prompt.encode()).hexdigest()\n",
    "    \n",
    "    def _optimize_prompt(self, prompt, max_tokens=4000):\n",
    "        \"\"\"Optimize a prompt to reduce token usage.\"\"\"\n",
    "        if len(prompt) <= max_tokens:\n",
    "            return prompt\n",
    "        \n",
    "        # Simple truncation strategy\n",
    "        return prompt[:max_tokens] + \"... [truncated]\"\n",
    "    \n",
    "    def generate(self, prompt, use_cache=True):\n",
    "        \"\"\"Generate a response with caching and performance tracking.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Optimize the prompt\n",
    "        optimized_prompt = self._optimize_prompt(prompt)\n",
    "        \n",
    "        # Check cache if enabled\n",
    "        if self.cache_enabled and use_cache:\n",
    "            cache_key = self._get_cache_key(optimized_prompt)\n",
    "            if cache_key in self.cache:\n",
    "                print(f\"[{self.name}] Cache hit!\")\n",
    "                return self.cache[cache_key], 0, 0\n",
    "        \n",
    "        # Make the API call\n",
    "        try:\n",
    "            self.request_count += 1\n",
    "            response = self.llm.invoke([HumanMessage(content=optimized_prompt)])\n",
    "            content = response.content\n",
    "            \n",
    "            # Estimate token count (very rough estimation)\n",
    "            estimated_tokens = len(optimized_prompt.split()) + len(content.split())\n",
    "            self.token_count += estimated_tokens\n",
    "            \n",
    "            # Cache the result if enabled\n",
    "            if self.cache_enabled and use_cache:\n",
    "                cache_key = self._get_cache_key(optimized_prompt)\n",
    "                self.cache[cache_key] = content\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            return content, estimated_tokens, elapsed_time\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            return f\"Error: {str(e)}\", 0, time.time() - start_time\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get performance statistics.\"\"\"\n",
    "        return {\n",
    "            \"name\": self.name,\n",
    "            \"requests\": self.request_count,\n",
    "            \"estimated_tokens\": self.token_count,\n",
    "            \"cache_size\": len(self.cache),\n",
    "            \"cache_enabled\": self.cache_enabled\n",
    "        }\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear the response cache.\"\"\"\n",
    "        cache_size = len(self.cache)\n",
    "        self.cache = {}\n",
    "        return f\"Cleared {cache_size} items from cache.\"\n",
    "\n",
    "# Create an optimized agent\n",
    "agent = OptimizedAgent(\"OptimizedAssistant\")\n",
    "\n",
    "# Example 1: First request (no cache)\n",
    "prompt1 = \"Explain the concept of quantum computing in simple terms.\"\n",
    "response1, tokens1, time1 = agent.generate(prompt1)\n",
    "\n",
    "print(f\"Response 1 (took {time1:.2f}s, ~{tokens1} tokens):\")\n",
    "print(response1[:300] + \"...\" if len(response1) > 300 else response1)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Example 2: Repeated request (should use cache)\n",
    "response2, tokens2, time2 = agent.generate(prompt1)\n",
    "\n",
    "print(f\"Response 2 (took {time2:.2f}s, ~{tokens2} tokens):\")\n",
    "print(f\"Cache performance improvement: {time1/time2:.2f}x faster\")\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Example 3: New request with long prompt (will be optimized)\n",
    "long_prompt = \"Explain the history of artificial intelligence\" + \" very thoroughly and in detail\" * 100\n",
    "response3, tokens3, time3 = agent.generate(long_prompt)\n",
    "\n",
    "print(f\"Response 3 (took {time3:.2f}s, ~{tokens3} tokens):\")\n",
    "print(response3[:300] + \"...\" if len(response3) > 300 else response3)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Print stats\n",
    "print(\"Agent Statistics:\")\n",
    "print(json.dumps(agent.get_stats(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac9078",
   "metadata": {},
   "source": [
    "## Example 5: Multi-Framework Pipeline for Document Analysis\n",
    "\n",
    "This example demonstrates a pipeline that uses multiple frameworks for different stages of document analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema import HumanMessage, Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, List, Dict, Any\n",
    "import autogen\n",
    "\n",
    "# Sample document\n",
    "sample_document = \"\"\"\n",
    "# Artificial Intelligence: A Modern Approach\n",
    "\n",
    "## Introduction\n",
    "Artificial Intelligence (AI) is a field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, and language understanding.\n",
    "\n",
    "## History of AI\n",
    "The field of AI research was founded at a workshop held at Dartmouth College in 1956. The attendees, including John McCarthy, Marvin Minsky, Allen Newell, and Herbert Simon, became the founders and leaders of AI research. They and their students produced programs that were described by the press as \"astonishing\": computers were solving word problems in algebra, proving logical theorems, and speaking English.\n",
    "\n",
    "By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense, and AI laboratories had been established around the world. AI had achieved success in limited domains, but had difficulty with more general problems.\n",
    "\n",
    "## Machine Learning\n",
    "Machine learning is a subset of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "### Types of Machine Learning\n",
    "1. **Supervised Learning**: The algorithm is trained on labeled data.\n",
    "2. **Unsupervised Learning**: The algorithm finds patterns in unlabeled data.\n",
    "3. **Reinforcement Learning**: The algorithm learns through trial and error, guided by rewards.\n",
    "\n",
    "## Natural Language Processing\n",
    "Natural Language Processing (NLP) is a field of AI that deals with the interaction between computers and humans using natural language. The ultimate objective of NLP is to read, understand, and generate human languages in a valuable way.\n",
    "\n",
    "## Computer Vision\n",
    "Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.\n",
    "\n",
    "## Ethical Considerations\n",
    "As AI systems become more powerful, ethical considerations become increasingly important. Key concerns include:\n",
    "\n",
    "- Privacy and surveillance\n",
    "- Bias and fairness\n",
    "- Accountability and transparency\n",
    "- Economic impact and job displacement\n",
    "\n",
    "## Future Directions\n",
    "Future research in AI will likely focus on creating more general systems that can perform a wide variety of tasks, rather than specialized systems for specific applications. Artificial General Intelligence (AGI) remains a long-term goal of many researchers.\n",
    "\"\"\"\n",
    "\n",
    "class DocumentState(TypedDict):\n",
    "    document: str\n",
    "    chunks: List[str]\n",
    "    topics: List[str]\n",
    "    summaries: Dict[str, str]\n",
    "    final_analysis: str\n",
    "\n",
    "# Define the document analysis pipeline\n",
    "class DocumentAnalysisPipeline:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI()\n",
    "        self.embeddings = OpenAIEmbeddings()\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        \n",
    "        # Set up AutoGen agents\n",
    "        config_list = [\n",
    "            {\n",
    "                'model': 'gpt-4',\n",
    "                'api_key': os.environ.get(\"OPENAI_API_KEY\")\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.analyst_agent = autogen.AssistantAgent(\n",
    "            name=\"Document_Analyst\",\n",
    "            llm_config={\"config_list\": config_list},\n",
    "            system_message=\"You are an expert at analyzing documents and extracting key insights.\"\n",
    "        )\n",
    "        \n",
    "        self.user_proxy = autogen.UserProxyAgent(\n",
    "            name=\"User_Proxy\",\n",
    "            human_input_mode=\"NEVER\"\n",
    "        )\n",
    "        \n",
    "        # Build LangGraph workflow\n",
    "        self.workflow = self._build_workflow()\n",
    "    \n",
    "    def _build_workflow(self):\n",
    "        workflow = StateGraph(DocumentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"chunk_document\", self.chunk_document)\n",
    "        workflow.add_node(\"extract_topics\", self.extract_topics)\n",
    "        workflow.add_node(\"generate_summaries\", self.generate_summaries)\n",
    "        workflow.add_node(\"analyze_document\", self.analyze_document)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.add_edge(\"chunk_document\", \"extract_topics\")\n",
    "        workflow.add_edge(\"extract_topics\", \"generate_summaries\")\n",
    "        workflow.add_edge(\"generate_summaries\", \"analyze_document\")\n",
    "        \n",
    "        # Set entry point\n",
    "        workflow.set_entry_point(\"chunk_document\")\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    # LangChain-based document chunking\n",
    "    def chunk_document(self, state: DocumentState) -> DocumentState:\n",
    "        chunks = self.text_splitter.split_text(state[\"document\"])\n",
    "        state[\"chunks\"] = chunks\n",
    "        return state\n",
    "    \n",
    "    # LLM-based topic extraction\n",
    "    def extract_topics(self, state: DocumentState) -> DocumentState:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze this document and identify the main topics covered:\n",
    "        \n",
    "        {state['document']}\n",
    "        \n",
    "        Return ONLY a list of 3-7 main topics as bullet points.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        topics = [line.strip().replace('- ', '').replace('* ', '') \n",
    "                 for line in response.content.strip().split('\\n') \n",
    "                 if line.strip() and (line.strip().startswith('- ') or line.strip().startswith('* '))]        \n",
    "        state[\"topics\"] = topics\n",
    "        return state\n",
    "    \n",
    "    # Generate topic-specific summaries using AutoGen\n",
    "    def generate_summaries(self, state: DocumentState) -> DocumentState:\n",
    "        summaries = {}\n",
    "        \n",
    "        for topic in state[\"topics\"]:\n",
    "            # Use AutoGen for detailed analysis of each topic\n",
    "            self.user_proxy.initiate_chat(\n",
    "                self.analyst_agent,\n",
    "                message=f\"Analyze this document regarding the topic '{topic}':\\n\\n{state['document']}\\n\\nProvide a concise summary of how this topic is covered.\"\n",
    "            )\n",
    "            \n",
    "            conversation = self.user_proxy.chat_messages[self.analyst_agent]\n",
    "            topic_summary = conversation[-1][\"content\"]\n",
    "            summaries[topic] = topic_summary\n",
    "        \n",
    "        state[\"summaries\"] = summaries\n",
    "        return state\n",
    "    \n",
    "    # LangGraph-based final analysis\n",
    "    def analyze_document(self, state: DocumentState) -> DocumentState:\n",
    "        topics_text = \"\\n\".join([f\"- {topic}\" for topic in state[\"topics\"]])\n",
    "        summaries_text = \"\\n\\n\".join([f\"## {topic}\\n{summary}\" for topic, summary in state[\"summaries\"].items()])\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Based on the analysis of the document, create a comprehensive overview.\n",
    "        \n",
    "        Main topics identified:\n",
    "        {topics_text}\n",
    "        \n",
    "        Topic summaries:\n",
    "        {summaries_text}\n",
    "        \n",
    "        Please provide a final analysis that includes:\n",
    "        1. An overview of the document's key themes\n",
    "        2. The relationships between different topics\n",
    "        3. The most important insights from the document\n",
    "        4. Any notable gaps or areas for further exploration\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke([HumanMessage(content=prompt)])\n",
    "        state[\"final_analysis\"] = response.content\n",
    "        return state\n",
    "    \n",
    "    def analyze(self, document: str) -> Dict[str, Any]:\n",
    "        result = self.workflow.invoke({\n",
    "            \"document\": document,\n",
    "            \"chunks\": [],\n",
    "            \"topics\": [],\n",
    "            \"summaries\": {},\n",
    "            \"final_analysis\": \"\"\n",
    "        })\n",
    "        return result\n",
    "\n",
    "# Run the document analysis pipeline\n",
    "pipeline = DocumentAnalysisPipeline()\n",
    "analysis_result = pipeline.analyze(sample_document)\n",
    "\n",
    "print(\"\\n=== DOCUMENT ANALYSIS RESULTS ===\")\n",
    "print(\"\\nIdentified Topics:\")\n",
    "for topic in analysis_result[\"topics\"]:\n",
    "    print(f\"- {topic}\")\n",
    "\n",
    "print(\"\\nTopic Summaries (excerpt from first topic):\")\n",
    "first_topic = analysis_result[\"topics\"][0]\n",
    "first_summary = analysis_result[\"summaries\"][first_topic]\n",
    "print(f\"## {first_topic}\")\n",
    "print(first_summary[:200] + \"...\" if len(first_summary) > 200 else first_summary)\n",
    "print(\"...and summaries for other topics...\")\n",
    "\n",
    "print(\"\\nFinal Analysis:\")\n",
    "print(analysis_result[\"final_analysis\"][:500] + \"...\" if len(analysis_result[\"final_analysis\"]) > 500 else analysis_result[\"final_analysis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95253c5",
   "metadata": {},
   "source": [
    "## Conclusion and Best Practices\n",
    "\n",
    "These examples demonstrate various approaches to building hybrid AI agent systems that combine multiple frameworks. When building your own hybrid systems, consider these best practices:\n",
    "\n",
    "1. **Choose frameworks based on strengths**: Select each framework for what it does best\n",
    "2. **Design clean interfaces**: Create clear boundaries between framework-specific components\n",
    "3. **Implement proper error handling**: Account for failures across framework boundaries\n",
    "4. **Monitor performance**: Track resource usage and response times for optimization\n",
    "5. **Build incrementally**: Start simple and add complexity as needed\n",
    "6. **Document integration points**: Make clear how data flows between framework components\n",
    "\n",
    "As the field of AI agents continues to evolve, hybrid approaches that combine the strengths of different frameworks will likely become increasingly common in production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc272ce2",
   "metadata": {},
   "source": [
    "## Pattern 2: CrewAI + AutoGen\n",
    "\n",
    "This example demonstrates how to combine CrewAI's role-based agents with AutoGen's conversational capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b73ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent as CrewAgent, Task, Crew\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "# Create CrewAI agents using our LLM client\n",
    "researcher = CrewAgent(\n",
    "    role=\"Research Analyst\",\n",
    "    goal=\"Find comprehensive information on given topics\",\n",
    "    backstory=\"You are an expert researcher with 15 years of experience in data analysis.\",\n",
    "    llm=clients[\"langchain_llm\"]  # Using the selected LLM\n",
    ")\n",
    "\n",
    "writer = CrewAgent(\n",
    "    role=\"Content Writer\",\n",
    "    goal=\"Transform research into engaging content\",\n",
    "    backstory=\"You are a skilled writer who can explain complex topics clearly.\",\n",
    "    llm=clients[\"langchain_llm\"]  # Using the selected LLM\n",
    ")\n",
    "\n",
    "# Create AutoGen agents using our AutoGen config\n",
    "assistant = AssistantAgent(\n",
    "    name=\"AI_Assistant\",\n",
    "    llm_config=clients[\"autogen_config\"],\n",
    "    system_message=\"You help users refine their research questions and provide guidance.\"\n",
    ")\n",
    "\n",
    "user_proxy = UserProxyAgent(\n",
    "    name=\"User\",\n",
    "    human_input_mode=\"NEVER\"  # Don't ask for human input in this example\n",
    ")\n",
    "\n",
    "# Function to connect CrewAI and AutoGen\n",
    "def hybrid_research_process(topic):\n",
    "    \"\"\"Combine CrewAI and AutoGen to research a topic and create content.\"\"\"\n",
    "    # Step 1: Use AutoGen to refine the research question\n",
    "    question_refinement = user_proxy.initiate_chat(\n",
    "        assistant,\n",
    "        message=f\"Help me refine this research topic: '{topic}'. Please suggest a more specific research question.\"\n",
    "    )\n",
    "    \n",
    "    # Extract the refined question (in a real implementation, you would parse the chat history)\n",
    "    refined_question = f\"Refined version of: {topic}\"  # Placeholder\n",
    "    \n",
    "    # Step 2: Use CrewAI for research and content creation\n",
    "    research_task = Task(\n",
    "        description=f\"Research this topic thoroughly: {refined_question}. Find key information, statistics, and insights.\",\n",
    "        agent=researcher\n",
    "    )\n",
    "    \n",
    "    writing_task = Task(\n",
    "        description=\"Using the research findings, create an engaging article that explains the topic clearly.\",\n",
    "        agent=writer,\n",
    "        context=[research_task]  # This task depends on the research task\n",
    "    )\n",
    "    \n",
    "    # Form a crew with these agents and tasks\n",
    "    crew = Crew(\n",
    "        agents=[researcher, writer],\n",
    "        tasks=[research_task, writing_task]\n",
    "    )\n",
    "    \n",
    "    # Execute the workflow (commented out)\n",
    "    # result = crew.kickoff()\n",
    "    # return result\n",
    "    \n",
    "    # For demonstration, return a placeholder\n",
    "    return f\"Research and content creation process completed for: {refined_question}\"\n",
    "\n",
    "# Example usage (commented out)\n",
    "'''\n",
    "result = hybrid_research_process(\"The impact of artificial intelligence on healthcare\")\n",
    "print(result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3412819e",
   "metadata": {},
   "source": [
    "## Pattern 3: Custom Orchestration Layer\n",
    "\n",
    "This example demonstrates how to create a custom orchestration layer that integrates multiple frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf91515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrchestrator:\n",
    "    \"\"\"A custom orchestrator that integrates multiple agent frameworks.\"\"\"\n",
    "    \n",
    "    def __init__(self, provider=\"openai\"):\n",
    "        # Set up clients based on provider choice\n",
    "        self.clients = create_clients(provider)\n",
    "        \n",
    "        # Set up components from different frameworks\n",
    "        self._setup_langchain_components()\n",
    "        self._setup_autogen_components()\n",
    "        self._setup_crewai_components()\n",
    "        self._setup_langgraph_workflow()\n",
    "        \n",
    "    def _setup_langchain_components(self):\n",
    "        \"\"\"Set up LangChain tools and components.\"\"\"\n",
    "        from langchain.agents import Tool\n",
    "        from langchain.utilities import GoogleSearchAPIWrapper\n",
    "        \n",
    "        # Mock search for demonstration\n",
    "        self.search = MockSearch()\n",
    "        self.langchain_tools = [\n",
    "            Tool(\n",
    "                name=\"Search\",\n",
    "                func=self.search.run,\n",
    "                description=\"Useful for searching the internet\"\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def _setup_autogen_components(self):\n",
    "        \"\"\"Set up AutoGen agents.\"\"\"\n",
    "        self.autogen_assistant = AssistantAgent(\n",
    "            name=\"AI_Assistant\",\n",
    "            llm_config=self.clients[\"autogen_config\"],\n",
    "            system_message=\"You help refine questions and provide guidance.\"\n",
    "        )\n",
    "        \n",
    "        self.autogen_user = UserProxyAgent(\n",
    "            name=\"User\",\n",
    "            human_input_mode=\"NEVER\"  # Don't ask for human input in this example\n",
    "        )\n",
    "    \n",
    "    def _setup_crewai_components(self):\n",
    "        \"\"\"Set up CrewAI agents and crew.\"\"\"\n",
    "        self.researcher = CrewAgent(\n",
    "            role=\"Research Analyst\",\n",
    "            goal=\"Find comprehensive information\",\n",
    "            backstory=\"You are an expert researcher.\",\n",
    "            llm=self.clients[\"langchain_llm\"]\n",
    "        )\n",
    "        \n",
    "        self.writer = CrewAgent(\n",
    "            role=\"Content Writer\",\n",
    "            goal=\"Create engaging content\",\n",
    "            backstory=\"You excel at creating readable content.\",\n",
    "            llm=self.clients[\"langchain_llm\"]\n",
    "        )\n",
    "        \n",
    "        # Tasks will be created dynamically based on the query\n",
    "        \n",
    "    def _setup_langgraph_workflow(self):\n",
    "        \"\"\"Set up LangGraph workflow.\"\"\"\n",
    "        # Define a simple workflow that will be used for processing\n",
    "        # This is a placeholder - in a real implementation, you would define a more complex graph\n",
    "        workflow = StateGraph(ResearchState)\n",
    "        workflow.add_node(\"search\", search_for_information)\n",
    "        workflow.add_node(\"analyze\", analyze_search_results)\n",
    "        workflow.compile()\n",
    "        self.workflow = workflow\n",
    "    \n",
    "    def process_task(self, task_type, query):\n",
    "        \"\"\"Process a task using the appropriate framework.\"\"\"\n",
    "        if task_type == \"research\":\n",
    "            return self._handle_research_task(query)\n",
    "        elif task_type == \"content_creation\":\n",
    "            return self._handle_content_task(query)\n",
    "        elif task_type == \"conversation\":\n",
    "            return self._handle_conversation_task(query)\n",
    "        else:\n",
    "            return f\"Unknown task type: {task_type}\"\n",
    "    \n",
    "    def _handle_research_task(self, query):\n",
    "        \"\"\"Handle research tasks using LangGraph + LangChain.\"\"\"\n",
    "        initial_state = {\"query\": query, \"search_results\": \"\", \"refined_results\": \"\", \"final_answer\": \"\"}\n",
    "        # In a real implementation, you would run the graph\n",
    "        # result = self.workflow.invoke(initial_state)\n",
    "        # return result[\"final_answer\"]\n",
    "        return f\"Research results for: {query} (using LangGraph + LangChain)\"\n",
    "    \n",
    "    def _handle_content_task(self, query):\n",
    "        \"\"\"Handle content creation tasks using CrewAI.\"\"\"\n",
    "        research_task = Task(\n",
    "            description=f\"Research this topic: {query}\",\n",
    "            agent=self.researcher\n",
    "        )\n",
    "        \n",
    "        writing_task = Task(\n",
    "            description=f\"Create content about: {query}\",\n",
    "            agent=self.writer,\n",
    "            context=[research_task]\n",
    "        )\n",
    "        \n",
    "        crew = Crew(\n",
    "            agents=[self.researcher, self.writer],\n",
    "            tasks=[research_task, writing_task]\n",
    "        )\n",
    "        \n",
    "        # In a real implementation, you would run the crew\n",
    "        # result = crew.kickoff()\n",
    "        # return result\n",
    "        return f\"Content created for: {query} (using CrewAI)\"\n",
    "    \n",
    "    def _handle_conversation_task(self, query):\n",
    "        \"\"\"Handle conversation tasks using AutoGen.\"\"\"\n",
    "        # In a real implementation, you would initiate a conversation\n",
    "        # self.autogen_user.initiate_chat(\n",
    "        #     self.autogen_assistant,\n",
    "        #     message=query\n",
    "        # )\n",
    "        return f\"Conversation about: {query} (using AutoGen)\"\n",
    "\n",
    "# Example usage (commented out)\n",
    "'''\n",
    "# Create the orchestrator\n",
    "orchestrator = CustomOrchestrator(provider=\"openai\")  # Change to \"azure\" to use Azure OpenAI\n",
    "\n",
    "# Process different types of tasks\n",
    "research_result = orchestrator.process_task(\"research\", \"What are the emerging trends in quantum computing?\")\n",
    "print(research_result)\n",
    "\n",
    "content_result = orchestrator.process_task(\"content_creation\", \"The future of renewable energy\")\n",
    "print(content_result)\n",
    "\n",
    "conversation_result = orchestrator.process_task(\"conversation\", \"Explain the concept of artificial general intelligence\")\n",
    "print(conversation_result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25cc0b",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored how to build hybrid AI agent systems that combine multiple frameworks:\n",
    "\n",
    "1. **LangChain + LangGraph**: Leveraging LangChain's rich ecosystem of tools with LangGraph's sophisticated control flow\n",
    "\n",
    "2. **CrewAI + AutoGen**: Combining CrewAI's intuitive role definitions with AutoGen's powerful conversation mechanisms\n",
    "\n",
    "3. **Custom Orchestration**: Building a custom orchestration layer that selectively uses different frameworks based on the task\n",
    "\n",
    "These hybrid approaches allow you to leverage the strengths of different frameworks while avoiding their individual limitations, creating more powerful and flexible AI agent systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
