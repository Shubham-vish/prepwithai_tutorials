{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22a61cb",
   "metadata": {},
   "source": [
    "# LangGraph Examples: Building Advanced AI Agent Workflows\n",
    "\n",
    "This notebook provides hands-on examples of using LangGraph for building sophisticated AI agent workflows with complex control flow. You can run these examples to gain practical experience with this powerful framework.\n",
    "\n",
    "## Helpful Resources\n",
    "\n",
    "- [Official LangGraph Documentation](https://python.langchain.com/docs/langgraph)\n",
    "- [LangGraph GitHub Repository](https://github.com/langchain-ai/langgraph)\n",
    "- [LangGraph Concepts Guide](https://python.langchain.com/docs/langgraph/concepts)\n",
    "- [LangGraph Tutorial](https://python.langchain.com/docs/langgraph/quick_start)\n",
    "- [LangGraph API Reference](https://api.python.langchain.com/en/latest/langgraph_api_reference.html)\n",
    "- [LangChain Blog: LangGraph](https://blog.langchain.dev/langgraph/)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e03074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langgraph langchain openai langchain_openai wikipedia\n",
    "\n",
    "# Optional packages for visualization\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346375b",
   "metadata": {},
   "source": [
    "## Setting API Keys and Configurations\n",
    "\n",
    "For these examples to work, you'll need to set your API keys for OpenAI or Azure OpenAI. Replace the placeholders with your actual API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Option 1: OpenAI Setup\n",
    "# Set your OpenAI API key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"  # Replace with your actual API key\n",
    "\n",
    "# Option 2: Azure OpenAI Setup\n",
    "# Set your Azure OpenAI variables\n",
    "# os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your-azure-openai-api-key\"  # Replace with your Azure API key\n",
    "# os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-resource-name.openai.azure.com/\"  # Replace with your endpoint\n",
    "\n",
    "# Function to create LLM based on provider choice\n",
    "def get_llm(provider=\"openai\", temperature=0.0):\n",
    "    if provider == \"azure\":\n",
    "        from langchain_openai import AzureChatOpenAI\n",
    "        return AzureChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            azure_deployment=\"gpt-4\",  # The deployment name you chose when you deployed the GPT-4 model\n",
    "            openai_api_version=\"2023-05-15\",\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "        )\n",
    "    else:  # Default to OpenAI\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        return ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "# Choose your provider here\n",
    "provider = \"openai\"  # Change to \"azure\" to use Azure OpenAI\n",
    "\n",
    "# Get appropriate LLM\n",
    "llm = get_llm(provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b723c4c",
   "metadata": {},
   "source": [
    "## Example 1: Basic Graph with Conditional Paths\n",
    "\n",
    "Let's start with a simple graph that demonstrates conditional branching based on question difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6222c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import Dict, TypedDict\n",
    "import json\n",
    "\n",
    "# Define our state using TypedDict for type hints\n",
    "class QuestionState(TypedDict):\n",
    "    question: str\n",
    "    difficulty: str\n",
    "    response: str\n",
    "\n",
    "# Initialize the configurable LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Define nodes in the graph\n",
    "def analyze_question(state: QuestionState) -> QuestionState:\n",
    "    \"\"\"Analyze the question and determine its difficulty.\"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=f\"Analyze this question and classify it as 'easy', 'medium', or 'hard': {state['question']}\")])\n",
    "    state[\"difficulty\"] = response.content.strip().lower()\n",
    "    return state\n",
    "\n",
    "def answer_easy_question(state: QuestionState) -> QuestionState:\n",
    "    \"\"\"Process easy questions.\"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=f\"Please answer this easy question briefly: {state['question']}\")])\n",
    "    state[\"response\"] = response.content\n",
    "    return state\n",
    "\n",
    "def answer_complex_question(state: QuestionState) -> QuestionState:\n",
    "    \"\"\"Process medium or hard questions with more detail.\"\"\"\n",
    "    response = llm.invoke([HumanMessage(content=f\"Please answer this {state['difficulty']} question in detail, with examples: {state['question']}\")])\n",
    "    state[\"response\"] = response.content\n",
    "    return state\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(QuestionState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"analyze_question\", analyze_question)\n",
    "workflow.add_node(\"answer_easy\", answer_easy_question)\n",
    "workflow.add_node(\"answer_complex\", answer_complex_question)\n",
    "\n",
    "# Define conditional routing\n",
    "def route_by_difficulty(state: QuestionState) -> str:\n",
    "    if \"easy\" in state[\"difficulty\"].lower():\n",
    "        return \"answer_easy\"\n",
    "    else:\n",
    "        return \"answer_complex\"\n",
    "\n",
    "# Add conditional edge\n",
    "workflow.add_conditional_edges(\"analyze_question\", route_by_difficulty)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"analyze_question\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Visualize the graph (if graphviz is installed)\n",
    "try:\n",
    "    graph.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")\n",
    "\n",
    "# Execute the graph with a sample question\n",
    "result = graph.invoke({\"question\": \"What is the capital of France?\", \"difficulty\": \"\", \"response\": \"\"})\n",
    "\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Classified as: {result['difficulty']}\")\n",
    "print(f\"\\nResponse: {result['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078be499",
   "metadata": {},
   "source": [
    "## Example 2: Cyclic Graph for Iterative Process\n",
    "\n",
    "Now let's create a more complex graph with cyclic behavior for iterative reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c018c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import Dict, List, TypedDict\n",
    "\n",
    "# Define our state using TypedDict\n",
    "class SolverState(TypedDict):\n",
    "    problem: str\n",
    "    solution: str\n",
    "    iterations: int\n",
    "    max_iterations: int\n",
    "    complete: bool\n",
    "\n",
    "# Define nodes in the graph\n",
    "def solve_step(state: SolverState) -> SolverState:\n",
    "    \"\"\"Generate or improve a solution.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    # First iteration: create initial solution\n",
    "    if state[\"iterations\"] == 0:\n",
    "        prompt = f\"Generate an initial solution for this problem: {state['problem']}\"\n",
    "    # Subsequent iterations: refine the solution\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "        Problem: {state['problem']}\n",
    "        Current solution (iteration {state['iterations']}): {state['solution']}\n",
    "        \n",
    "        Improve the solution above by being more precise, addressing more edge cases, or fixing any issues.\n",
    "        \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"solution\"] = response.content\n",
    "    return state\n",
    "\n",
    "def evaluate_solution(state: SolverState) -> SolverState:\n",
    "    \"\"\"Evaluate if the solution is complete or needs improvement.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    # Increment the iteration counter\n",
    "    state[\"iterations\"] += 1\n",
    "    \n",
    "    # If we've reached max iterations, mark as complete\n",
    "    if state[\"iterations\"] >= state[\"max_iterations\"]:\n",
    "        state[\"complete\"] = True\n",
    "        return state\n",
    "        \n",
    "    # Otherwise, evaluate the solution quality\n",
    "    prompt = f\"\"\"\n",
    "    Problem: {state['problem']}\n",
    "    Current solution: {state['solution']}\n",
    "    \n",
    "    Evaluate this solution. Is it complete and optimal? \n",
    "    Answer only 'COMPLETE' if the solution is excellent and needs no further improvement.\n",
    "    Answer 'INCOMPLETE' if the solution could be improved further.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"complete\"] = \"COMPLETE\" in response.content.upper()\n",
    "    return state\n",
    "\n",
    "# Define condition for the loop\n",
    "def should_continue(state: SolverState) -> str:\n",
    "    \"\"\"Determine whether to continue iterating or finish.\"\"\"\n",
    "    if state[\"complete\"]:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"solve_step\"\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(SolverState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"solve_step\", solve_step)\n",
    "workflow.add_node(\"evaluate\", evaluate_solution)\n",
    "\n",
    "# Add edges, including the cycle\n",
    "workflow.add_edge(\"solve_step\", \"evaluate\")\n",
    "workflow.add_conditional_edges(\"evaluate\", should_continue)\n",
    "workflow.add_edge(\"end\", None)  # End node\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"solve_step\")\n",
    "\n",
    "# Visualize the cyclic graph\n",
    "try:\n",
    "    graph = workflow.compile()\n",
    "    graph.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")\n",
    "    graph = workflow.compile()\n",
    "\n",
    "# Execute the graph with a sample problem\n",
    "result = graph.invoke({\n",
    "    \"problem\": \"Design an algorithm to find the maximum sum subarray in a given array of integers.\",\n",
    "    \"solution\": \"\",\n",
    "    \"iterations\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"complete\": False\n",
    "})\n",
    "\n",
    "print(f\"Problem: {result['problem']}\")\n",
    "print(f\"Completed in {result['iterations']} iterations\")\n",
    "print(f\"\\nFinal Solution: {result['solution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c12c11c",
   "metadata": {},
   "source": [
    "## Example 3: Decision-Making Agent with Research Capabilities\n",
    "\n",
    "Let's create a more practical example of an agent that can research information and make decisions based on that research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8762b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from typing import Dict, List, TypedDict, Optional\n",
    "import textwrap\n",
    "\n",
    "# Set up the Wikipedia tool\n",
    "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "# Define our state using TypedDict\n",
    "class ResearchState(TypedDict):\n",
    "    query: str\n",
    "    research_needed: bool\n",
    "    research_results: Optional[str]\n",
    "    follow_up_questions: List[str]\n",
    "    final_answer: str\n",
    "\n",
    "# Define nodes in the graph\n",
    "def analyze_query(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Analyze the query to determine if research is needed.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Query: {state['query']}\n",
    "    \n",
    "    Does this query require looking up factual information that might not be in your training data?\n",
    "    Answer with just 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"research_needed\"] = response.content.strip().lower() == \"yes\"\n",
    "    return state\n",
    "\n",
    "def perform_research(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Use Wikipedia to research the query.\"\"\"\n",
    "    try:\n",
    "        result = wikipedia.run(state[\"query\"])\n",
    "        # Truncate very long results\n",
    "        if len(result) > 3000:\n",
    "            result = result[:3000] + \"... [truncated]\"\n",
    "        state[\"research_results\"] = result\n",
    "    except Exception as e:\n",
    "        state[\"research_results\"] = f\"Error performing research: {str(e)}\"\n",
    "    return state\n",
    "\n",
    "def generate_follow_up_questions(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Generate follow-up questions based on research.\"\"\"\n",
    "    if not state[\"research_needed\"] or not state[\"research_results\"]:\n",
    "        state[\"follow_up_questions\"] = []\n",
    "        return state\n",
    "    \n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on the following research results for the query: {state['query']}\n",
    "    \n",
    "    Research: {state['research_results']}\n",
    "    \n",
    "    Generate 3 specific follow-up questions that would help clarify or expand on this information.\n",
    "    Return ONLY the questions as a numbered list.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    # Parse the questions (assuming they're numbered)\n",
    "    lines = response.content.strip().split('\\n')\n",
    "    questions = [line.strip() for line in lines if line.strip()]\n",
    "    state[\"follow_up_questions\"] = questions\n",
    "    return state\n",
    "\n",
    "def formulate_answer(state: ResearchState) -> ResearchState:\n",
    "    \"\"\"Generate a comprehensive answer.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    if state[\"research_needed\"] and state[\"research_results\"]:\n",
    "        prompt = f\"\"\"\n",
    "        Query: {state['query']}\n",
    "        Research results: {state['research_results']}\n",
    "        \n",
    "        Based on this research, provide a comprehensive and accurate answer to the original query.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        prompt = f\"Please answer this query based on your knowledge: {state['query']}\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"final_answer\"] = response.content\n",
    "    return state\n",
    "\n",
    "# Define routing logic\n",
    "def route_after_analysis(state: ResearchState) -> str:\n",
    "    if state[\"research_needed\"]:\n",
    "        return \"perform_research\"\n",
    "    else:\n",
    "        return \"formulate_answer\"\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"analyze_query\", analyze_query)\n",
    "workflow.add_node(\"perform_research\", perform_research)\n",
    "workflow.add_node(\"generate_follow_up_questions\", generate_follow_up_questions)\n",
    "workflow.add_node(\"formulate_answer\", formulate_answer)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_conditional_edges(\"analyze_query\", route_after_analysis)\n",
    "workflow.add_edge(\"perform_research\", \"generate_follow_up_questions\")\n",
    "workflow.add_edge(\"generate_follow_up_questions\", \"formulate_answer\")\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"analyze_query\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Try to visualize\n",
    "try:\n",
    "    graph.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")\n",
    "\n",
    "# Execute the graph with a query that likely needs research\n",
    "result = graph.invoke({\n",
    "    \"query\": \"What were the key contributions of Marie Curie to science?\", \n",
    "    \"research_needed\": False, \n",
    "    \"research_results\": None,\n",
    "    \"follow_up_questions\": [],\n",
    "    \"final_answer\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"Research needed: {result['research_needed']}\")\n",
    "print(\"\\nFollow-up questions:\")\n",
    "for i, question in enumerate(result['follow_up_questions'], 1):\n",
    "    print(f\"{i}. {question}\")\n",
    "print(\"\\nFinal answer:\")\n",
    "print(textwrap.fill(result['final_answer'], width=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fccd5cc",
   "metadata": {},
   "source": [
    "## Example 4: Multi-Agent System with State Persistence\n",
    "\n",
    "Let's create a complex multi-agent system where different specialized agents collaborate with persistent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2659bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from typing import Dict, List, TypedDict, Optional, Any\n",
    "import json\n",
    "\n",
    "# Define our state using TypedDict\n",
    "class ProjectState(TypedDict):\n",
    "    task: str\n",
    "    current_phase: str\n",
    "    requirements: List[str]\n",
    "    design: str\n",
    "    implementation: str\n",
    "    feedback: List[str]\n",
    "    final_solution: str\n",
    "    complete: bool\n",
    "    \n",
    "# Define agent roles and their corresponding functions\n",
    "\n",
    "def product_manager(state: ProjectState) -> ProjectState:\n",
    "    \"\"\"Product manager agent that defines requirements.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a Product Manager responsible for defining clear requirements.\n",
    "    \n",
    "    Task: {state['task']}\n",
    "    \n",
    "    Please define 3-5 specific, measurable requirements for this task.\n",
    "    Format each requirement as a separate point.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    lines = response.content.strip().split('\\n')\n",
    "    requirements = [line.strip() for line in lines if line.strip()]\n",
    "    state[\"requirements\"] = requirements\n",
    "    state[\"current_phase\"] = \"requirements_defined\"\n",
    "    return state\n",
    "\n",
    "def designer(state: ProjectState) -> ProjectState:\n",
    "    \"\"\"Designer agent that creates a high-level design.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    requirements_text = \"\\n\".join([f\"- {req}\" for req in state[\"requirements\"]])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a Designer responsible for creating a high-level design based on requirements.\n",
    "    \n",
    "    Task: {state['task']}\n",
    "    \n",
    "    Requirements:\n",
    "    {requirements_text}\n",
    "    \n",
    "    Please create a high-level design that addresses these requirements.\n",
    "    Include key components, their interactions, and any important design patterns or principles.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"design\"] = response.content\n",
    "    state[\"current_phase\"] = \"design_created\"\n",
    "    return state\n",
    "\n",
    "def developer(state: ProjectState) -> ProjectState:\n",
    "    \"\"\"Developer agent that implements the design.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    requirements_text = \"\\n\".join([f\"- {req}\" for req in state[\"requirements\"]])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a Developer responsible for implementing the design.\n",
    "    \n",
    "    Task: {state['task']}\n",
    "    \n",
    "    Requirements:\n",
    "    {requirements_text}\n",
    "    \n",
    "    Design:\n",
    "    {state['design']}\n",
    "    \n",
    "    Please provide an implementation (code or pseudocode) that follows this design \n",
    "    and meets all the requirements. Focus on clarity and correctness.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"implementation\"] = response.content\n",
    "    state[\"current_phase\"] = \"implementation_complete\"\n",
    "    return state\n",
    "\n",
    "def reviewer(state: ProjectState) -> ProjectState:\n",
    "    \"\"\"Reviewer agent that evaluates the implementation.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    requirements_text = \"\\n\".join([f\"- {req}\" for req in state[\"requirements\"]])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a Reviewer responsible for evaluating the implementation against requirements.\n",
    "    \n",
    "    Task: {state['task']}\n",
    "    \n",
    "    Requirements:\n",
    "    {requirements_text}\n",
    "    \n",
    "    Implementation:\n",
    "    {state['implementation']}\n",
    "    \n",
    "    Please review the implementation and provide 2-3 specific pieces of feedback.\n",
    "    Focus on whether it meets all requirements and follows the design.\n",
    "    Format each feedback point separately.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    lines = response.content.strip().split('\\n')\n",
    "    feedback = [line.strip() for line in lines if line.strip()]\n",
    "    state[\"feedback\"] = feedback\n",
    "    state[\"current_phase\"] = \"review_complete\"\n",
    "    return state\n",
    "\n",
    "def integrator(state: ProjectState) -> ProjectState:\n",
    "    \"\"\"Integrator agent that finalizes the solution based on feedback.\"\"\"\n",
    "    llm = ChatOpenAI()\n",
    "    \n",
    "    feedback_text = \"\\n\".join([f\"- {fb}\" for fb in state[\"feedback\"]])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are an Integrator responsible for finalizing the solution based on all previous work.\n",
    "    \n",
    "    Task: {state['task']}\n",
    "    Implementation: {state['implementation']}\n",
    "    Feedback: \n",
    "    {feedback_text}\n",
    "    \n",
    "    Please create a final solution that incorporates the feedback and ensures all requirements are met.\n",
    "    This should be the complete, polished solution ready for delivery.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"final_solution\"] = response.content\n",
    "    state[\"complete\"] = True\n",
    "    state[\"current_phase\"] = \"complete\"\n",
    "    return state\n",
    "\n",
    "# Define workflow transitions based on current phase\n",
    "def route_by_phase(state: ProjectState) -> str:\n",
    "    current_phase = state[\"current_phase\"]\n",
    "    \n",
    "    phase_routing = {\n",
    "        \"start\": \"product_manager\",\n",
    "        \"requirements_defined\": \"designer\",\n",
    "        \"design_created\": \"developer\",\n",
    "        \"implementation_complete\": \"reviewer\",\n",
    "        \"review_complete\": \"integrator\",\n",
    "        \"complete\": \"end\"\n",
    "    }\n",
    "    \n",
    "    return phase_routing.get(current_phase, \"product_manager\")\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(ProjectState)\n",
    "\n",
    "# Add nodes for each agent role\n",
    "workflow.add_node(\"product_manager\", product_manager)\n",
    "workflow.add_node(\"designer\", designer)\n",
    "workflow.add_node(\"developer\", developer)\n",
    "workflow.add_node(\"reviewer\", reviewer)\n",
    "workflow.add_node(\"integrator\", integrator)\n",
    "\n",
    "# Add conditional routing based on the current phase\n",
    "workflow.add_conditional_edges(\"start\", route_by_phase)\n",
    "workflow.add_conditional_edges(\"product_manager\", route_by_phase)\n",
    "workflow.add_conditional_edges(\"designer\", route_by_phase)\n",
    "workflow.add_conditional_edges(\"developer\", route_by_phase)\n",
    "workflow.add_conditional_edges(\"reviewer\", route_by_phase)\n",
    "workflow.add_conditional_edges(\"integrator\", route_by_phase)\n",
    "workflow.add_edge(\"end\", None)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"start\")\n",
    "\n",
    "# Compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Try to visualize\n",
    "try:\n",
    "    graph.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize graph: {e}\")\n",
    "\n",
    "# Execute the graph with a sample task\n",
    "result = graph.invoke({\n",
    "    \"task\": \"Create a simple todo list application with add, remove, and mark-complete functionality\",\n",
    "    \"current_phase\": \"start\",\n",
    "    \"requirements\": [],\n",
    "    \"design\": \"\",\n",
    "    \"implementation\": \"\",\n",
    "    \"feedback\": [],\n",
    "    \"final_solution\": \"\",\n",
    "    \"complete\": False\n",
    "})\n",
    "\n",
    "print(f\"Task: {result['task']}\")\n",
    "print(\"\\nRequirements:\")\n",
    "for req in result['requirements']:\n",
    "    print(f\"- {req}\")\n",
    "    \n",
    "print(\"\\nDesign:\")\n",
    "print(result['design'][:300] + \"...\" if len(result['design']) > 300 else result['design'])\n",
    "\n",
    "print(\"\\nImplementation (excerpt):\")\n",
    "print(result['implementation'][:300] + \"...\" if len(result['implementation']) > 300 else result['implementation'])\n",
    "\n",
    "print(\"\\nFeedback:\")\n",
    "for fb in result['feedback']:\n",
    "    print(f\"- {fb}\")\n",
    "    \n",
    "print(\"\\nFinal Solution (excerpt):\")\n",
    "print(result['final_solution'][:300] + \"...\" if len(result['final_solution']) > 300 else result['final_solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4092034",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "These examples demonstrate the core capabilities of LangGraph for creating complex, stateful workflows. For more advanced usage, consider exploring:\n",
    "\n",
    "1. **Persistent State Management**: Using databases or file systems to maintain state between sessions\n",
    "2. **Advanced Visualization**: Creating custom visualizations of complex workflows\n",
    "3. **Integration with External Systems**: Connecting nodes to APIs, databases, etc.\n",
    "4. **Nested Graphs**: Creating hierarchical workflows with graphs inside nodes\n",
    "5. **Error Handling**: Implementing robust error recovery mechanisms\n",
    "\n",
    "Check out the [LangGraph documentation](https://python.langchain.com/docs/langgraph) for more examples and advanced usage scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed029413",
   "metadata": {},
   "source": [
    "## Using Azure OpenAI with LangGraph\n",
    "\n",
    "This example demonstrates how to explicitly create and use Azure OpenAI with LangGraph for a cyclic reasoning pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure OpenAI specific setup\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Create Azure OpenAI LLM instance (comment out if you don't have Azure OpenAI access)\n",
    "azure_llm = AzureChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    azure_deployment=\"gpt-4\",  # Your deployment name\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://your-resource-name.openai.azure.com/\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"your-azure-openai-api-key\")\n",
    ")\n",
    "\n",
    "# Define our state\n",
    "class IterativeState(TypedDict):\n",
    "    problem: str\n",
    "    solution: str\n",
    "    iterations: int\n",
    "    max_iterations: int\n",
    "    complete: bool\n",
    "\n",
    "# Define nodes for our reasoning graph\n",
    "def refine_solution(state: IterativeState):\n",
    "    \"\"\"Refine the current solution.\"\"\"\n",
    "    # Using Azure OpenAI for this function\n",
    "    prompt = f\"\"\"\n",
    "    Problem: {state['problem']}\n",
    "    Current solution: {state['solution']}\n",
    "    \n",
    "    You are in iteration {state['iterations']} of solving this problem.\n",
    "    Please refine and improve the solution, making it more detailed and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Uncomment the next two lines to use Azure OpenAI when properly configured\n",
    "    # response = azure_llm.invoke([HumanMessage(content=prompt)])\n",
    "    # state[\"solution\"] = response.content\n",
    "    \n",
    "    # For demonstration, use the regular LLM instead\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    state[\"solution\"] = response.content\n",
    "    \n",
    "    return state\n",
    "\n",
    "def evaluate_solution(state: IterativeState):\n",
    "    \"\"\"Evaluate if the solution is complete.\"\"\"\n",
    "    state[\"iterations\"] += 1\n",
    "    \n",
    "    if state[\"iterations\"] >= state[\"max_iterations\"]:\n",
    "        state[\"complete\"] = True\n",
    "        return state\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Problem: {state['problem']}\n",
    "    Current solution: {state['solution']}\n",
    "    \n",
    "    Is this solution complete and optimal? Answer YES if it is complete, or NO with a brief explanation if it needs further refinement.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Uncomment the next line to use Azure OpenAI when properly configured\n",
    "    # response = azure_llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    # For demonstration, use the regular LLM instead\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    if \"YES\" in response.content.upper() or state[\"iterations\"] >= state[\"max_iterations\"]:\n",
    "        state[\"complete\"] = True\n",
    "    else:\n",
    "        state[\"complete\"] = False\n",
    "        \n",
    "    return state\n",
    "\n",
    "# Create the graph\n",
    "workflow = StateGraph(IterativeState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"refine\", refine_solution)\n",
    "workflow.add_node(\"evaluate\", evaluate_solution)\n",
    "\n",
    "# Add edges with conditional logic\n",
    "workflow.add_edge(\"refine\", \"evaluate\")\n",
    "workflow.add_edge(\"evaluate\", \"refine\", condition=lambda state: not state[\"complete\"])\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"refine\")\n",
    "\n",
    "# Compile the graph\n",
    "iterative_graph = workflow.compile()\n",
    "\n",
    "# For demonstration purposes (uncomment to execute)\n",
    "'''\n",
    "# Create initial state\n",
    "initial_state = {\n",
    "    \"problem\": \"Design an efficient algorithm to find the longest palindromic substring in a string.\",\n",
    "    \"solution\": \"We can use a simple approach by checking all possible substrings.\",\n",
    "    \"iterations\": 0,\n",
    "    \"max_iterations\": 3,\n",
    "    \"complete\": False\n",
    "}\n",
    "\n",
    "# Execute the graph\n",
    "result = iterative_graph.invoke(initial_state)\n",
    "\n",
    "print(f\"Final solution after {result['iterations']} iterations:\")\n",
    "print(result['solution'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdea38c",
   "metadata": {},
   "source": [
    "## Complete Example with Configurable LLM Choice\n",
    "\n",
    "This example brings everything together in a complete implementation that can easily switch between OpenAI and Azure OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f3b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Implementation with Provider Choice\n",
    "\n",
    "def run_research_agent(query, provider=\"openai\", max_iterations=3):\n",
    "    \"\"\"Run a research agent workflow with the specified LLM provider.\"\"\"\n",
    "    # Get the right LLM\n",
    "    if provider == \"azure\":\n",
    "        from langchain_openai import AzureChatOpenAI\n",
    "        llm = AzureChatOpenAI(\n",
    "            temperature=0.7,\n",
    "            azure_deployment=\"gpt-4\",\n",
    "            openai_api_version=\"2023-05-15\",\n",
    "            azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"\"),\n",
    "            api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"\")\n",
    "        )\n",
    "    else:  # Default to OpenAI\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4\",\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    # Define our state\n",
    "    class ResearchState(TypedDict):\n",
    "        query: str\n",
    "        plan: str\n",
    "        research_notes: str\n",
    "        final_report: str\n",
    "        iterations: int\n",
    "        max_iterations: int\n",
    "        complete: bool\n",
    "    \n",
    "    # Define nodes\n",
    "    def create_plan(state: ResearchState):\n",
    "        \"\"\"Create a research plan based on the query.\"\"\"\n",
    "        prompt = f\"Create a detailed research plan for investigating: {state['query']}\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        state[\"plan\"] = response.content\n",
    "        return state\n",
    "    \n",
    "    def conduct_research(state: ResearchState):\n",
    "        \"\"\"Conduct research based on the plan.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Research Query: {state['query']}\n",
    "        Research Plan: {state['plan']}\n",
    "        Current Research Notes: {state.get('research_notes', '')}\n",
    "        Iteration: {state['iterations'] + 1} of {state['max_iterations']}\n",
    "        \n",
    "        Based on the above, conduct the next phase of research. Add new information and insights not already covered.\n",
    "        \"\"\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        # Append new research to existing notes\n",
    "        if 'research_notes' not in state or not state['research_notes']:\n",
    "            state['research_notes'] = response.content\n",
    "        else:\n",
    "            state['research_notes'] = state['research_notes'] + \"\\n\\n\" + response.content\n",
    "            \n",
    "        state['iterations'] += 1\n",
    "        return state\n",
    "    \n",
    "    def evaluate_research(state: ResearchState):\n",
    "        \"\"\"Evaluate if research is complete.\"\"\"\n",
    "        if state['iterations'] >= state['max_iterations']:\n",
    "            state['complete'] = True\n",
    "            return state\n",
    "            \n",
    "        prompt = f\"\"\"\n",
    "        Research Query: {state['query']}\n",
    "        Research Plan: {state['plan']}\n",
    "        Current Research Notes: {state['research_notes']}\n",
    "        \n",
    "        Is the research complete enough to write a comprehensive report? Answer YES or NO.\n",
    "        If NO, briefly explain what aspects need more investigation.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        if \"YES\" in response.content.upper():\n",
    "            state['complete'] = True\n",
    "        else:\n",
    "            state['complete'] = False\n",
    "            \n",
    "        return state\n",
    "    \n",
    "    def generate_report(state: ResearchState):\n",
    "        \"\"\"Generate final research report.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Research Query: {state['query']}\n",
    "        Research Plan: {state['plan']}\n",
    "        Research Notes: {state['research_notes']}\n",
    "        \n",
    "        Create a comprehensive, well-structured final report based on the research conducted.\n",
    "        The report should be informative, factual, and directly address the original query.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        state['final_report'] = response.content\n",
    "        return state\n",
    "    \n",
    "    # Create the graph\n",
    "    workflow = StateGraph(ResearchState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"create_plan\", create_plan)\n",
    "    workflow.add_node(\"conduct_research\", conduct_research)\n",
    "    workflow.add_node(\"evaluate_research\", evaluate_research)\n",
    "    workflow.add_node(\"generate_report\", generate_report)\n",
    "    \n",
    "    # Add edges with conditional logic\n",
    "    workflow.add_edge(\"create_plan\", \"conduct_research\")\n",
    "    workflow.add_edge(\"conduct_research\", \"evaluate_research\")\n",
    "    workflow.add_edge(\"evaluate_research\", \"conduct_research\", condition=lambda state: not state[\"complete\"])\n",
    "    workflow.add_edge(\"evaluate_research\", \"generate_report\", condition=lambda state: state[\"complete\"])\n",
    "    \n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"create_plan\")\n",
    "    \n",
    "    # Compile the graph\n",
    "    research_graph = workflow.compile()\n",
    "    \n",
    "    # Create initial state\n",
    "    initial_state = {\n",
    "        \"query\": query,\n",
    "        \"plan\": \"\",\n",
    "        \"research_notes\": \"\",\n",
    "        \"final_report\": \"\",\n",
    "        \"iterations\": 0,\n",
    "        \"max_iterations\": max_iterations,\n",
    "        \"complete\": False\n",
    "    }\n",
    "    \n",
    "    # Run the graph\n",
    "    return research_graph.invoke(initial_state)\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "'''\n",
    "query = \"What are the most promising applications of graph neural networks in drug discovery?\"\n",
    "provider = \"openai\"  # Change to \"azure\" to use Azure OpenAI\n",
    "result = run_research_agent(query, provider=provider, max_iterations=2)\n",
    "\n",
    "print(\"========== FINAL RESEARCH REPORT ==========\\n\")\n",
    "print(result[\"final_report\"])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa7ce4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored LangGraph's powerful features for building complex AI agent workflows with advanced control flow. We've demonstrated:\n",
    "\n",
    "1. Setting up LangGraph with both OpenAI and Azure OpenAI\n",
    "2. Creating conditional branching in workflows based on LLM decisions\n",
    "3. Implementing cyclic reasoning patterns with state management\n",
    "4. Building a complete research agent with configurable LLM backend\n",
    "\n",
    "The examples show how LangGraph enables sophisticated graph-based architectures that can represent complex, non-linear workflows with advanced state management capabilities."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
