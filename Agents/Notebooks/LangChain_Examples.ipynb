{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "490b6472",
   "metadata": {},
   "source": [
    "# LangChain Examples: Building AI Agent Applications\n",
    "\n",
    "This notebook provides hands-on examples of using LangChain for building various types of AI agent applications. You can run these examples to gain practical experience with the framework.\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "First, let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain openai chromadb tiktoken\n",
    "\n",
    "# For the search example, you'll need to uncomment and install this package\n",
    "# !pip install google-search-results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95bb9c2",
   "metadata": {},
   "source": [
    "## Setting API Keys\n",
    "\n",
    "For these examples to work, you'll need to set your API keys for OpenAI and other services. Replace the placeholder with your actual API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5aabd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# =========================\n",
    "# ðŸ” API Keys & Endpoints\n",
    "# =========================\n",
    "\n",
    "# OpenAI / Azure OpenAI Configuration\n",
    "# For OpenAI or Azure OpenAI auth\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"\n",
    "# Azure OpenAI key\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your-azure-openai-api-key-here\"\n",
    "# Azure endpoint (replace with your endpoint)\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-resource-name.openai.azure.com\"\n",
    "# Often same as endpoint\n",
    "os.environ[\"AZURE_API_BASE\"] = \"https://your-resource-name.openai.azure.com\"\n",
    "# API version\n",
    "os.environ[\"AZURE_API_VERSION\"] = \"2024-02-15-preview\"\n",
    "# Azure deployment name (e.g., gpt-4o)\n",
    "os.environ[\"AZURE_DEPLOYMENT_NAME\"] = \"your-deployment-name-here\"\n",
    "\n",
    "# General OpenAI Config (if using `openai` package with Azure backend)\n",
    "# Required for Azure OpenAI\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-02-15-preview\"\n",
    "os.environ[\"MODEL_NAME\"] = \"gpt-4o\"\n",
    "os.environ[\"MODEL_TEMPERATURE\"] = \"0\"\n",
    "# For Google's Gemini API (if used)\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your-gemini-api-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e4d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4b4a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Azure OpenAI\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your-azure-openai-key-here\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://your-resource-name.openai.azure.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65df393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_model = \"your-default-model-here\"  # e.g., \"gpt-4o\"\n",
    "default_model = \"gpt-4o\"\n",
    "\n",
    "model_name = os.getenv(\"MODEL_NAME\", default_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18c1ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env found at: /home/shubham/prepwithai_tutorials/.env\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import sys\n",
    "\n",
    "\n",
    "dotenv_path = find_dotenv()\n",
    "if dotenv_path:\n",
    "    print(f\".env found at: {dotenv_path}\")\n",
    "    load_dotenv(dotenv_path, override=True)\n",
    "else:\n",
    "    print(\"âš ï¸ .env file not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e9bda66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important OS Environment Variables:\n",
      "AZURE_OPENAI_API_KEY: b160b4c8493a4584b729e06cef70a80b\n",
      "AZURE_OPENAI_ENDPOINT: https://topic-navigator.openai.azure.com\n",
      "MODEL_NAME: gpt-4o\n",
      "MODEL_TEMPERATURE: 0\n"
     ]
    }
   ],
   "source": [
    "# check important os values\n",
    "print(\"Important OS Environment Variables:\")\n",
    "print(f\"AZURE_OPENAI_API_KEY: {os.getenv('AZURE_OPENAI_API_KEY')}\")\n",
    "print(f\"AZURE_OPENAI_ENDPOINT: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"MODEL_NAME: {os.getenv('MODEL_NAME')}\")\n",
    "print(f\"MODEL_TEMPERATURE: {os.getenv('MODEL_TEMPERATURE')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aa3686b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Variables:\n",
      "AZURE_OPENAI_DEPLOYMENT_NAME: your-deployment-name-here\n",
      "AZURE_OPENAI_API_TYPE: azure\n",
      "AZURE_OPENAI_API_VERSION: 2024-02-15-preview\n",
      "OPENAI_API_KEY: your-openai-api-key-here\n",
      "MPLBACKEND: module://matplotlib_inline.backend_inline\n",
      "GIT_PAGER: cat\n",
      "PAGER: cat\n",
      "CLICOLOR_FORCE: 1\n",
      "FORCE_COLOR: 1\n",
      "CLICOLOR: 1\n",
      "TERM: xterm-color\n",
      "PYDEVD_USE_FRAME_EVAL: NO\n",
      "PYTHON_FROZEN_MODULES: on\n",
      "PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING: 1\n",
      "AZURE_OPENAI_ENDPOINT: https://topic-navigator.openai.azure.com\n",
      "PS1: (venv) \n",
      "OPENAI_API_TYPE: azure\n",
      "VIRTUAL_ENV_PROMPT: (venv) \n",
      "PYTHONIOENCODING: utf-8\n",
      "OPENAI_API_VERSION: 2024-02-15-preview\n",
      "SERPAPI_API_KEY: your_serpapi_key_here\n",
      "VIRTUAL_ENV: /home/shubham/prepwithai_tutorials/venv\n",
      "AZURE_API_BASE: https://your-resource-name.openai.azure.com\n",
      "AZURE_OPENAI_API_KEY: b160b4c8493a4584b729e06cef70a80b\n",
      "MODEL_TEMPERATURE: 0\n",
      "TAVILY_API_KEY: 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\n",
      "GEMINI_API_KEY: AIzaSyCgSThJCQP2svFVD7vcE4c9qwWZuvnJj7o\n",
      "DEPLOYMENT_NAME: gpt-4o\n",
      "AZURE_DEPLOYMENT_NAME: your-deployment-name-here\n",
      "AZURE_API_VERSION: 2024-02-15-preview\n",
      "PYTHONUNBUFFERED: 1\n",
      "MODEL_NAME: gpt-4o\n",
      "VSCODE_L10N_BUNDLE_LOCATION: \n",
      "VSCODE_IPC_HOOK_CLI: /run/user/1000/vscode-ipc-fe3f091c-e4bc-49ab-914d-5ba9838213ba.sock\n",
      "ELECTRON_RUN_AS_NODE: 1\n",
      "BROWSER: /home/shubham/.vscode-server/cli/servers/Stable-19e0f9e681ecb8e5c09d8784acaa601316ca4571/server/bin/helpers/browser.sh\n",
      "VSCODE_HANDLES_UNCAUGHT_ERRORS: true\n",
      "VSCODE_ESM_ENTRYPOINT: vs/workbench/api/node/extensionHostProcess\n",
      "XDG_DATA_DIRS: /usr/local/share:/usr/share:/var/lib/snapd/desktop\n",
      "LESSOPEN: | /usr/bin/lesspipe %s\n",
      "LESSCLOSE: /usr/bin/lesspipe %s %s\n",
      "SGX_AESM_ADDR: 1\n",
      "LS_COLORS: \n",
      "VSCODE_HANDLES_SIGPIPE: true\n",
      "VSCODE_NLS_CONFIG: {\"userLocale\":\"en\",\"osLocale\":\"en\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"/home/shubham/.vscode-server/cli/servers/Stable-19e0f9e681ecb8e5c09d8784acaa601316ca4571/server/out/nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}\n",
      "VSCODE_CWD: /home/shubham\n",
      "SSH_CONNECTION: 106.222.228.241 10050 10.0.0.4 22\n",
      "PWD: /home/shubham\n",
      "SHELL: /bin/bash\n",
      "LANG: C.UTF-8\n",
      "SSL_CERT_DIR: /usr/lib/ssl/certs\n",
      "XDG_RUNTIME_DIR: /run/user/1000\n",
      "VSCODE_AGENT_FOLDER: /home/shubham/.vscode-server\n",
      "PATH: /home/shubham/prepwithai_tutorials/venv/bin:/home/shubham/.vscode-server/cli/servers/Stable-19e0f9e681ecb8e5c09d8784acaa601316ca4571/server/bin/remote-cli:/home/shubham/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n",
      "VSCODE_CLI_REQUIRE_TOKEN: 6eb32b9b-7188-4695-8411-b5d6f12af826\n",
      "XDG_SESSION_ID: 535\n",
      "XDG_SESSION_CLASS: user\n",
      "_: /home/shubham/prepwithai_tutorials/venv/bin/python\n",
      "LOGNAME: shubham\n",
      "DBUS_SESSION_BUS_ADDRESS: unix:path=/run/user/1000/bus\n",
      "SSL_CERT_FILE: /usr/lib/ssl/cert.pem\n",
      "HOME: /home/shubham\n",
      "SHLVL: 0\n",
      "XDG_SESSION_TYPE: tty\n",
      "SSH_CLIENT: 106.222.228.241 10050 22\n",
      "USER: shubham\n"
     ]
    }
   ],
   "source": [
    "# check by printing all the environment variables in reverse order\n",
    "# This is just for debugging purposes\n",
    "print(\"Environment Variables:\")\n",
    "for key, value in reversed(list(os.environ.items())):\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "996c57ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    model=os.getenv(\"MODEL_NAME\", model_name),\n",
    "    temperature=float(os.getenv(\"MODEL_TEMPERATURE\", \"0\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8cffbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297409e",
   "metadata": {},
   "source": [
    "## Example 1: Basic Chain\n",
    "\n",
    "Let's start with a simple chain that demonstrates the core functionality of LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e58a86b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.chains'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchains\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMChain\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain.chains'"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Define a prompt template\n",
    "template = \"Explain {concept} in simple terms a 10-year-old would understand.\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"concept\"])\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(temperature=0.7)\n",
    "\n",
    "# Create the chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain\n",
    "response = chain.run(concept=\"artificial intelligence\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13f276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "135a05ba",
   "metadata": {},
   "source": [
    "## Example 2: Sequential Chain\n",
    "\n",
    "Now let's explore a sequential chain where the output of one step becomes the input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# First chain: Generate an idea\n",
    "first_prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a creative business idea related to {topic}.\"\n",
    ")\n",
    "first_chain = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# Second chain: Analyze the idea\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"business_idea\"],\n",
    "    template=\"Analyze the strengths and weaknesses of this business idea: {business_idea}\"\n",
    ")\n",
    "second_chain = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "# Create the sequential chain\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[first_chain, second_chain],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the chain\n",
    "response = overall_chain.run(\"sustainable fashion\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197944d",
   "metadata": {},
   "source": [
    "## Example 3: RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Let's implement a basic RAG system that can answer questions based on document content. First, we'll create a sample document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6098f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample document\n",
    "with open(\"sample_document.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "    # AI Agent Frameworks Overview\n",
    "    \n",
    "    ## LangChain\n",
    "    LangChain is a framework built around LLMs. It enables applications that are context-aware, reason, and learn from feedback. \n",
    "    The framework was created by Harrison Chase in October 2022 and has gained significant popularity.\n",
    "    \n",
    "    ## CrewAI\n",
    "    CrewAI is designed for orchestrating role-based autonomous AI agents. \n",
    "    It focuses on creating specialized agents that can collaborate as a cohesive team.\n",
    "    \n",
    "    ## AutoGen\n",
    "    AutoGen is a framework from Microsoft that enables the development of LLM applications using multiple conversational agents.\n",
    "    These agents can talk to each other to solve tasks, and can also converse with humans.\n",
    "    \n",
    "    ## LangGraph\n",
    "    LangGraph extends LangChain with cyclical graphs, enabling more complex, multi-step workflows and agent behaviors.\n",
    "    It's particularly useful for creating systems with feedback loops and iterative reasoning.\n",
    "    \"\"\")\n",
    "\n",
    "print(\"Sample document created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225895db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# Load the document\n",
    "loader = TextLoader('sample_document.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)\n",
    "\n",
    "# Create a retrieval QA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever()\n",
    ")\n",
    "\n",
    "# Ask questions about the document\n",
    "queries = [\n",
    "    \"When was LangChain created and by whom?\",\n",
    "    \"What is CrewAI designed for?\",\n",
    "    \"What is the main difference between LangGraph and LangChain?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuestion: {query}\")\n",
    "    print(f\"Answer: {qa.run(query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149b814",
   "metadata": {},
   "source": [
    "## Example 4: Memory in Chains\n",
    "\n",
    "Let's create a chain with memory to maintain conversation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e250ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Create a conversation with memory\n",
    "llm = OpenAI(temperature=0.7)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "response1 = conversation.predict(input=\"Hi, my name is Alice.\")\n",
    "print(\"Response 1:\", response1)\n",
    "\n",
    "# Second interaction - the model should remember the name\n",
    "response2 = conversation.predict(input=\"What's my name?\")\n",
    "print(\"Response 2:\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aec3a9",
   "metadata": {},
   "source": [
    "## Example 5: Simple Agent with Tools\n",
    "\n",
    "Finally, let's implement a simple agent that can use tools. \n",
    "Note: This example requires a SerpAPI key which you would need to uncomment and set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b12e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this code if you have a SerpAPI key set\n",
    "\"\"\"\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.tools import BaseTool\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "# Create a calculator tool\n",
    "class CalculatorTool(BaseTool):\n",
    "    name = \"Calculator\"\n",
    "    description = \"Useful for performing mathematical calculations\"\n",
    "    \n",
    "    def _run(self, query: str) -> str:\n",
    "        try:\n",
    "            return str(eval(query))\n",
    "        except Exception as e:\n",
    "            return f\"Error in calculation: {str(e)}\"\n",
    "    \n",
    "    async def _arun(self, query: str) -> str:\n",
    "        return self._run(query)\n",
    "\n",
    "# Set up search and calculator tools\n",
    "search = SerpAPIWrapper()\n",
    "calculator = CalculatorTool()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for searching the internet for current information\"\n",
    "    ),\n",
    "    calculator\n",
    "]\n",
    "\n",
    "# Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools, \n",
    "    OpenAI(temperature=0),\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run the agent with a complex query requiring tools\n",
    "agent.run(\"What is the square root of the population of France?\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0264b5e1",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "These examples just scratch the surface of what's possible with LangChain. For more advanced usage, consider exploring:\n",
    "\n",
    "1. **Custom tools and agents**: Create specialized tools for your specific domain\n",
    "2. **Advanced memory types**: Explore different memory implementations for various use cases\n",
    "3. **Integration with other frameworks**: Combine LangChain with frameworks like Streamlit or Gradio for UI\n",
    "4. **Custom chains**: Build your own chain types for specific applications\n",
    "\n",
    "Check out the [LangChain documentation](https://python.langchain.com/docs/get_started) for more examples and advanced usage scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
